train_config:
  batch_size: 16
  micro_batch_size: 8
  num_epochs: 1
  lr: !!float 1e-4
  weight_decay: !!float 0.0
  max_length: 256
  val_set_size: 0
  eval_step: 200
  save_step: 200
  use_gradient_checkpointing: false
  train_on_inputs: false

test_config:
  test_batch_size: 32
  test_dataset_ids: ""
  merge: true
  
lora_config:
  rank: 8
  lora_alpha: 16
  lora_dropout: !!float 0.05
  target_modules: "qkv"
  dtype: "bf16"
  
model: "NousResearch/Meta-Llama-3-8B"
data_path: "dataset/math_10k.json"
output_dir: "./log/lora"
seed: 42
finetune: true
evaluate: true
